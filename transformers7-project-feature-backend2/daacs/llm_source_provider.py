"""
DAACS v6.0 - LLM Source Provider
ì—­í• ë³„ë¡œ CLI Assistantì˜ LLM ë˜ëŠ” í”ŒëŸ¬ê·¸ì¸ LLMì„ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì¶”ìƒí™” ê³„ì¸µ
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import subprocess
import json


class LLMSource(ABC):
    """LLM ì†ŒìŠ¤ ë² ì´ìŠ¤ í´ë˜ìŠ¤ (CLI Assistant ë˜ëŠ” Plugin)"""

    @abstractmethod
    def invoke(self, prompt: str, **kwargs) -> str:
        """LLM í˜¸ì¶œ"""
        pass

    @abstractmethod
    def invoke_structured(self, prompt: str, schema: Optional[Dict] = None) -> Dict:
        """êµ¬ì¡°í™”ëœ ì¶œë ¥ (JSON)"""
        pass


class CLIAssistantLLMSource(LLMSource):
    """
    CLI Assistantì˜ ë‚´ì¥ LLM ì‚¬ìš©

    ì˜ˆ: Claude Codeë¥¼ ì‹¤í–‰í•˜ë©´ ë‚´ë¶€ì ìœ¼ë¡œ Claudeì˜ LLMì´ ë™ì‘
    Codexë¥¼ ì‹¤í–‰í•˜ë©´ ë‚´ë¶€ì ìœ¼ë¡œ GPTì˜ LLMì´ ë™ì‘
    """

    def __init__(
        self,
        cli_type: str,
        temperature: float = 0.7,
        timeout_sec: int = 60,
        fallback_config: Optional[Dict] = None
    ):
        """
        Args:
            cli_type: CLI Assistant íƒ€ì… (codex, claude_code, cursor, aider)
            temperature: LLM temperature
            timeout_sec: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            fallback_config: Fallback í”ŒëŸ¬ê·¸ì¸ LLM ì„¤ì •
        """
        self.cli_type = cli_type
        self.temperature = temperature
        self.timeout_sec = timeout_sec
        self.fallback_config = fallback_config
        
        # Initialize CodexClient
        from .llm.cli_executor import CodexClient
        self.client = CodexClient(
            cwd=".",  # Default to current directory or project root handled by client
            timeout_sec=timeout_sec,
            client_name="cli_assistant",
            cli_type=cli_type
        )

    def invoke(self, prompt: str, **kwargs) -> str:
        """CLI Assistant LLM í˜¸ì¶œ (ì‹¤íŒ¨ ì‹œ Fallback)"""

        try:
            # Use CodexClient to execute
            result = self.client.execute(prompt)
            
            if result.startswith("Error:") or result.startswith("Exception:"):
                raise RuntimeError(f"CLI Assistant failed: {result}")
                
            return result

        except Exception as e:
            print(f"[WARN] CLI Assistant LLM failed: {e}")

            # Fallback to Plugin LLM
            if self.fallback_config:
                print(f"ğŸ”„ Falling back to Plugin LLM ({self.fallback_config['provider']})")
                fallback_source = PluginLLMSource(
                    provider=self.fallback_config["provider"],
                    model=self.fallback_config.get("model", "gpt-5.1"),
                    temperature=self.temperature
                )
                return fallback_source.invoke(prompt, **kwargs)

            raise RuntimeError(f"CLI Assistant LLM unavailable and no fallback configured: {e}")

    def invoke_structured(self, prompt: str, schema: Optional[Dict] = None) -> Dict:
        """êµ¬ì¡°í™”ëœ ì¶œë ¥"""
        response = self.invoke(prompt + "\n\nRespond in JSON format.")

        # JSON íŒŒì‹± ì‹œë„
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            # JSONì´ ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸ë¥¼ ê°ì‹¸ì„œ ë°˜í™˜
            return {"response": response}


class PluginLLMSource(LLMSource):
    """
    í”ŒëŸ¬ê·¸ì¸ LLM ì‚¬ìš© (Groq, Claude, Gemini, GPT ë“±)
    ì‹¤ì œ êµ¬í˜„ì€ ì¶”í›„ LLM Registryì—ì„œ ë¡œë“œ
    """

    def __init__(
        self,
        provider: str,
        model: str,
        temperature: float = 0.7,
        api_key: Optional[str] = None
    ):
        """
        Args:
            provider: LLM í”„ë¡œë°”ì´ë” (groq, claude, gemini, openai)
            model: ëª¨ë¸ ì´ë¦„
            temperature: LLM temperature
            api_key: API í‚¤ (ì˜µì…˜, í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ)
        """
        self.provider = provider
        self.model = model
        self.temperature = temperature
        self.api_key = api_key

        # LLM í”ŒëŸ¬ê·¸ì¸ ì´ˆê¸°í™”
        self.llm = self._initialize_llm()

    def _initialize_llm(self):
        """LLM í”ŒëŸ¬ê·¸ì¸ ì´ˆê¸°í™”"""
        import os
        
        if self.provider == "openai":
            try:
                from openai import OpenAI
                api_key = self.api_key or os.environ.get("OPENAI_API_KEY")
                if not api_key:
                    print("[WARN] OPENAI_API_KEY not found. OpenAI plugin will fail.")
                    return None
                return OpenAI(api_key=api_key)
            except ImportError:
                print("[WARN] openai package not installed.")
                return None
            except Exception as e:
                print(f"[WARN] Failed to initialize OpenAI: {e}")
                return None
        
        elif self.provider == "anthropic":
            try:
                from anthropic import Anthropic
                api_key = self.api_key or os.environ.get("ANTHROPIC_API_KEY")
                if not api_key:
                    print("[WARN] ANTHROPIC_API_KEY not found. Anthropic plugin will fail.")
                    return None
                return Anthropic(api_key=api_key)
            except ImportError:
                print("[WARN] anthropic package not installed.")
                return None
            except Exception as e:
                print(f"[WARN] Failed to initialize Anthropic: {e}")
                return None
        
        elif self.provider == "gemini":
            try:
                import google.generativeai as genai
                api_key = self.api_key or os.environ.get("GOOGLE_API_KEY")
                if not api_key:
                    print("[WARN] GOOGLE_API_KEY not found. Gemini plugin will fail.")
                    return None
                genai.configure(api_key=api_key)
                return genai.GenerativeModel(self.model)
            except ImportError:
                print("[WARN] google-generativeai package not installed.")
                return None
            except Exception as e:
                print(f"[WARN] Failed to initialize Gemini: {e}")
                return None
                
        print(f"[PluginLLMSource] Initialized: {self.provider}/{self.model}")
        return None

    def invoke(self, prompt: str, **kwargs) -> str:
        """í”ŒëŸ¬ê·¸ì¸ LLM í˜¸ì¶œ"""
        if self.llm:
            try:
                if self.provider == "openai":
                    response = self.llm.chat.completions.create(
                        model=self.model,
                        messages=[{"role": "user", "content": prompt}],
                        temperature=self.temperature
                    )
                    return response.choices[0].message.content
                
                elif self.provider == "anthropic":
                    response = self.llm.messages.create(
                        model=self.model,
                        max_tokens=8000,
                        temperature=self.temperature,
                        messages=[{"role": "user", "content": prompt}]
                    )
                    return response.content[0].text
                
                elif self.provider == "gemini":
                    response = self.llm.generate_content(
                        prompt,
                        generation_config={"temperature": self.temperature}
                    )
                    return response.text
                
                # ë‹¤ë¥¸ í”„ë¡œë°”ì´ë”ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ
                return self.llm.invoke(prompt)
            except Exception as e:
                print(f"[WARN] Plugin execution failed: {e}")
                # Fallbackìœ¼ë¡œ ì§„í–‰

        # Fallback: v5.0 ë°©ì‹ìœ¼ë¡œ codex exec ì‚¬ìš©
        print(f"[WARN] Plugin LLM not implemented or failed, using codex fallback")
        try:
            result = subprocess.run(
                ["codex", "exec", prompt],
                capture_output=True,
                text=True,
                timeout=60,
                check=False
            )
            if result.returncode == 0:
                return result.stdout.strip()
            else:
                raise RuntimeError(f"Codex failed: {result.stderr}")
        except Exception as e:
            print(f"[WARN] Codex fallback failed: {e}")
            print(f"ğŸ”„ Falling back to Mock LLM")
            # Mockìœ¼ë¡œ Fallback (ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ìœ„í•´)
            mock = MockLLMSource(role="backend") # Roleì€ ì¶”ì •
            return mock.invoke(prompt)

    def invoke_structured(self, prompt: str, schema: Optional[Dict] = None) -> Dict:
        """êµ¬ì¡°í™”ëœ ì¶œë ¥"""
        if self.llm and hasattr(self.llm, 'invoke_structured'):
            return self.llm.invoke_structured(prompt, schema)

        # Fallback: JSON ìš”ì²­ í›„ íŒŒì‹±
        response = self.invoke(prompt + "\n\nRespond in JSON format.")
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            return {"response": response}



class MockLLMSource(LLMSource):
    """í…ŒìŠ¤íŠ¸ìš© Mock LLM"""

    def __init__(self, role: str):
        self.role = role

    def invoke(self, prompt: str, **kwargs) -> str:
        print(f"[MockLLM:{self.role}] Invoked with prompt length: {len(prompt)}")
        
        if self.role == "orchestrator":
            return """
Plan:
1. Backend: Create main.py and requirements.txt
2. Frontend: Create App.jsx and package.json
"""
        elif self.role == "backend":
            return """
FILE: main.py
```python
from fastapi import FastAPI
app = FastAPI()
@app.get("/")
def read_root():
    return {"Hello": "World"}
```

FILE: requirements.txt
```
fastapi
uvicorn
```
"""
        elif self.role == "frontend":
            return """
FILE: App.jsx
```javascript
import React from 'react';
function App() {
  return <h1>Hello World</h1>;
}
export default App;
```

FILE: package.json
```json
{
  "dependencies": {
    "react": "^18.2.0"
  }
}
```
"""
        return "Mock response"

    def invoke_structured(self, prompt: str, schema: Optional[Dict] = None) -> Dict:
        return {"response": self.invoke(prompt)}


class LLMSourceFactory:

    """ì—­í• ë³„ LLM ì†ŒìŠ¤ ìƒì„± íŒ©í† ë¦¬"""

    @staticmethod
    def create_from_config(role_config: Dict, cli_type: str, timeout_sec: int = 60) -> LLMSource:
        """
        ì„¤ì •ì—ì„œ LLM ì†ŒìŠ¤ ìƒì„±

        Args:
            role_config: ì—­í•  ì„¤ì • (orchestrator, backend, frontend)
                ì˜ˆ: {"source": "cli_assistant", "temperature": 0.7}
                ë˜ëŠ”: {"source": "plugin", "plugin": {"provider": "groq", "model": "llama-3.3-70b"}}
            cli_type: CLI Assistant íƒ€ì… (codex, claude_code, etc)
            timeout_sec: íƒ€ì„ì•„ì›ƒ (ì´ˆ)

        Returns:
            LLMSource ì¸ìŠ¤í„´ìŠ¤
        """
        source = role_config.get("source")
        temperature = role_config.get("temperature", 0.7)

        if source == "cli_assistant":
            # CLI Assistantì˜ ë‚´ì¥ LLM ì‚¬ìš©
            # Allow role-specific override for cli_type
            effective_cli_type = role_config.get("cli_type", cli_type)
            fallback = role_config.get("fallback")
            return CLIAssistantLLMSource(
                cli_type=effective_cli_type,
                temperature=temperature,
                timeout_sec=timeout_sec,
                fallback_config=fallback
            )

        elif source == "plugin":
            # í”ŒëŸ¬ê·¸ì¸ LLM ì‚¬ìš©
            plugin_config = role_config.get("plugin", {})
            return PluginLLMSource(
                provider=plugin_config.get("provider", "openai"),
                model=plugin_config.get("model", "gpt-5.1"),
                temperature=temperature,
                api_key=plugin_config.get("api_key")
            )

        elif source == "mock":
            # í…ŒìŠ¤íŠ¸ìš© Mock
            return MockLLMSource(role=role_config.get("role", "unknown"))

        else:
            raise ValueError(f"Unknown LLM source: {source}. Must be 'cli_assistant' or 'plugin'")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì˜ˆì‹œ 1: CLI Assistant LLM (Codex)
    cli_llm = CLIAssistantLLMSource(cli_type="codex", temperature=0.3)
    print("CLI Assistant LLM created")

    # ì˜ˆì‹œ 2: Plugin LLM (Groq)
    plugin_llm = PluginLLMSource(provider="groq", model="llama-3.3-70b-versatile", temperature=0.7)
    print("Plugin LLM created")

    # ì˜ˆì‹œ 3: Factory ì‚¬ìš©
    config = {
        "source": "cli_assistant",
        "temperature": 0.5,
        "fallback": {
            "provider": "claude",
            "model": "claude-sonnet-4.5"
        }
    }
    llm_source = LLMSourceFactory.create_from_config(config, cli_type="codex")
    print("LLM Source created from config:", type(llm_source))
